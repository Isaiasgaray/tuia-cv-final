{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_L36jBl7-WPI"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "if [[ ! -d \"dataset\" ]]; then\n",
        "  mkdir dataset\n",
        "  curl -L -o dataset/70-dog-breedsimage-data-set.zip\\\n",
        "    https://www.kaggle.com/api/v1/datasets/download/gpiosenka/70-dog-breedsimage-data-set\n",
        "  unzip dataset/70-dog-breedsimage-data-set.zip -d dataset\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "import os\n",
        "import copy\n",
        "import time\n",
        "from torch.optim import lr_scheduler\n",
        "from collections import Counter\n",
        "\n",
        "data_dir = 'dataset'\n",
        "data_phases = ['train', 'valid']\n",
        "\n",
        "# Definir transformaciones\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'valid': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n"
      ],
      "metadata": {
        "id": "ZMJoI6-v0ETG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los datasets usando ImageFolder\n",
        "image_datasets = {\n",
        "    x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
        "    for x in data_phases\n",
        "}\n",
        "\n",
        "# Crear DataLoaders\n",
        "dataloaders = {\n",
        "    x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4)\n",
        "    for x in data_phases\n",
        "}\n",
        "\n",
        "# Información del dataset (importante para el modelo)\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in data_phases}\n",
        "class_names = image_datasets['train'].classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Configuración del dispositivo (CPU/GPU)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1. Obtener los índices de clase de todas las imágenes de entrenamiento\n",
        "train_labels = [label for _, label in image_datasets['train'].samples]\n",
        "class_counts = Counter(train_labels)\n",
        "class_counts_list = [class_counts[i] for i in range(num_classes)]\n",
        "\n",
        "# 2. Calcular los pesos inversos\n",
        "total_samples = sum(class_counts_list)\n",
        "weights_list = [total_samples / count for count in class_counts_list]\n",
        "weights_tensor = torch.tensor(weights_list, dtype=torch.float32)\n",
        "\n",
        "# Definir la función de pérdida con pesos\n",
        "criterion = nn.CrossEntropyLoss(weight=weights_tensor.to(device))\n",
        "\n",
        "print(f\"Clases detectadas: {num_classes}\")\n",
        "print(f\"Imágenes de Entrenamiento: {dataset_sizes['train']}\")\n",
        "print(f\"Imágenes de Validación: {dataset_sizes['valid']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1pUaWAxyw5P",
        "outputId": "da68df4a-6fb8-4e4d-ff0a-4b4eb5dc7782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clases detectadas: 70\n",
            "Imágenes de Entrenamiento: 7946\n",
            "Imágenes de Validación: 700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Obtener los índices de clase de todas las imágenes de entrenamiento\n",
        "train_labels = [label for _, label in image_datasets['train'].samples]\n",
        "# 2. Contar las ocurrencias de cada clase\n",
        "class_counts = Counter(train_labels)\n",
        "# Asegurar que los conteos estén en el orden correcto de los índices de clase (0 a num_classes-1)\n",
        "class_counts_list = [class_counts[i] for i in range(num_classes)]\n",
        "\n",
        "# 3. Calcular los pesos inversos (pesos más altos para clases menos frecuentes)\n",
        "# El peso es inversamente proporcional a la frecuencia de la clase\n",
        "total_samples = sum(class_counts_list)\n",
        "weights_list = [total_samples / count for count in class_counts_list]\n",
        "\n",
        "# Normalizar los pesos (opcional, pero ayuda a la estabilidad)\n",
        "# weights_tensor = torch.tensor(weights_list, dtype=torch.float32)\n",
        "# weights_tensor = weights_tensor / weights_tensor.sum() * num_classes\n",
        "\n",
        "weights_tensor = torch.tensor(weights_list, dtype=torch.float32)\n",
        "\n",
        "print(f\"Pesos de clases (ejemplo): {weights_tensor[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNXOLwK1V_-w",
        "outputId": "d8c13f13-42a5-43a6-e5ca-358055b18789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pesos de clases (ejemplo): tensor([ 75.6762,  72.8991,  67.9145, 122.2462,  89.2809])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo ResNet18 pre-entrenado\n",
        "model_ft = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Reemplazar la capa final (fully-connected)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# La nueva capa de clasificación con Dropout\n",
        "model_ft.fc = nn.Sequential(\n",
        "    nn.Dropout(0.2), # Nueva capa de Dropout\n",
        "    nn.Linear(num_ftrs, num_classes)\n",
        ")\n",
        "\n",
        "try:\n",
        "    # --- PASO DE SOLUCIÓN DE ERRORES ---\n",
        "    # 1. Cargar el state_dict directamente (no en el modelo aún)\n",
        "    state_dict = torch.load('resnet18_70_breeds_best_weights_v2.pth')\n",
        "\n",
        "    # 2. Renombrar las claves antiguas a las nuevas claves secuenciales\n",
        "    # La clave 'fc.weight' debe ser 'fc.1.weight'\n",
        "    # La clave 'fc.bias' debe ser 'fc.1.bias'\n",
        "\n",
        "    # NOTA: Usamos .pop() para mover y eliminar las claves antiguas\n",
        "    state_dict['fc.1.weight'] = state_dict.pop('fc.weight')\n",
        "    state_dict['fc.1.bias'] = state_dict.pop('fc.bias')\n",
        "\n",
        "    # 3. Cargar el state_dict modificado en el modelo\n",
        "    model_ft.load_state_dict(state_dict)\n",
        "\n",
        "    print(\"Pesos del mejor modelo (0.9171 Acc) cargados exitosamente después de corregir el nombre de las capas.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ADVERTENCIA: Archivo de pesos no encontrado. Entrenando desde el inicio.\")\n",
        "\n",
        "\n",
        "# --- Bloque de Congelación Diferencial (se mantiene igual) ---\n",
        "\n",
        "for name, param in model_ft.named_parameters():\n",
        "    if name.startswith('fc'):\n",
        "        # Siempre entrenamos la capa final\n",
        "        param.requires_grad = True\n",
        "    elif name.startswith('layer3') or name.startswith('layer4'):\n",
        "        # Entrenamos las capas de alto nivel\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        # Congelamos conv1, bn1, layer1, layer2\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Verificar cuántos parámetros se van a entrenar\n",
        "params_to_update = [p for p in model_ft.parameters() if p.requires_grad]\n",
        "print(f\"Parámetros a entrenar: {len(params_to_update)} (Capas 3, 4 y FC)\")\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# --- Bloque del Optimizador (se mantiene igual) ---\n",
        "\n",
        "# Usamos un LR muy bajo, ya que el modelo ya está muy cerca de la solución.\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.00005, momentum=0.9) # LR ajustado a 0.00005"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2l9Me0FzEx4",
        "outputId": "820651c5-b147-489a-a50a-bab60f1e2518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pesos del mejor modelo (0.9171 Acc) cargados exitosamente después de corregir el nombre de las capas.\n",
            "Parámetros a entrenar: 32 (Capas 3, 4 y FC)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "  since = time.time()\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "\n",
        "  # Iterar sobre las épocas\n",
        "  for epoch in range(num_epochs):\n",
        "      print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "      print('-' * 20)\n",
        "\n",
        "      for phase in ['train', 'valid']:\n",
        "          if phase == 'train':\n",
        "              # El StepLR solía ir aquí. En ReduceLROnPlateau, va después de la validación.\n",
        "              model.train()\n",
        "          else:\n",
        "              model.eval()\n",
        "\n",
        "          running_loss = 0.0\n",
        "          running_corrects = 0\n",
        "\n",
        "          for inputs, labels in dataloaders[phase]:\n",
        "              inputs = inputs.to(device)\n",
        "              labels = labels.to(device)\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              with torch.set_grad_enabled(phase == 'train'):\n",
        "                  outputs = model(inputs)\n",
        "                  _, preds = torch.max(outputs, 1)\n",
        "                  loss = criterion(outputs, labels)\n",
        "\n",
        "                  if phase == 'train':\n",
        "                      loss.backward()\n",
        "                      optimizer.step()\n",
        "\n",
        "              running_loss += loss.item() * inputs.size(0)\n",
        "              running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "          epoch_loss = running_loss / dataset_sizes[phase]\n",
        "          epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "          print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "          # **CAMBIO 5: Aplicar scheduler.step() después de la fase 'valid'**\n",
        "          if phase == 'valid':\n",
        "              # El scheduler se ajusta basado en la precisión de validación (epoch_acc)\n",
        "              scheduler.step(epoch_acc)\n",
        "\n",
        "              if epoch_acc > best_acc:\n",
        "                  best_acc = epoch_acc\n",
        "                  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "  time_elapsed = time.time() - since\n",
        "  print(f'Entrenamiento completado en {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "  print(f'Mejor precisión de validación: {best_acc:.4f}')\n",
        "\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model"
      ],
      "metadata": {
        "id": "PTQ9oprQzdGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReduceLROnPlateau es un scheduler común para Fine-Tuning\n",
        "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer_ft,\n",
        "    mode='max',      # Observar la métrica que maximizamos (Acc)\n",
        "    factor=0.1,      # Reducir el LR a 1/10\n",
        "    patience=3      # Esperar 3 épocas sin mejora antes de reducir\n",
        ")\n",
        "\n",
        "# --- INICIO DEL ENTRENAMIENTO ---\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "model_ft = train_model(\n",
        "    model_ft,\n",
        "    criterion,\n",
        "    optimizer_ft,\n",
        "    exp_lr_scheduler,\n",
        "    num_epochs=NUM_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qskwHj6zzek",
        "outputId": "a94d9282-2a81-4495-abbc-c45c03f35d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/50\n",
            "--------------------\n",
            "train Loss: 0.9707 Acc: 0.7443\n",
            "valid Loss: 0.7125 Acc: 0.9071\n",
            "\n",
            "Epoch 2/50\n",
            "--------------------\n",
            "train Loss: 0.9584 Acc: 0.7546\n",
            "valid Loss: 0.7075 Acc: 0.9086\n",
            "\n",
            "Epoch 3/50\n",
            "--------------------\n",
            "train Loss: 0.9445 Acc: 0.7516\n",
            "valid Loss: 0.7058 Acc: 0.9171\n",
            "\n",
            "Epoch 4/50\n",
            "--------------------\n",
            "train Loss: 0.9351 Acc: 0.7531\n",
            "valid Loss: 0.7045 Acc: 0.9171\n",
            "\n",
            "Epoch 5/50\n",
            "--------------------\n",
            "train Loss: 0.9462 Acc: 0.7538\n",
            "valid Loss: 0.6920 Acc: 0.9129\n",
            "\n",
            "Epoch 6/50\n",
            "--------------------\n",
            "train Loss: 0.9230 Acc: 0.7581\n",
            "valid Loss: 0.6982 Acc: 0.9143\n",
            "\n",
            "Epoch 7/50\n",
            "--------------------\n",
            "train Loss: 0.9414 Acc: 0.7499\n",
            "valid Loss: 0.6884 Acc: 0.9200\n",
            "\n",
            "Epoch 8/50\n",
            "--------------------\n",
            "train Loss: 0.9081 Acc: 0.7692\n",
            "valid Loss: 0.6958 Acc: 0.9171\n",
            "\n",
            "Epoch 9/50\n",
            "--------------------\n",
            "train Loss: 0.9232 Acc: 0.7553\n",
            "valid Loss: 0.6916 Acc: 0.9129\n",
            "\n",
            "Epoch 10/50\n",
            "--------------------\n",
            "train Loss: 0.9091 Acc: 0.7610\n",
            "valid Loss: 0.7017 Acc: 0.9171\n",
            "\n",
            "Epoch 11/50\n",
            "--------------------\n",
            "train Loss: 0.8971 Acc: 0.7648\n",
            "valid Loss: 0.6990 Acc: 0.9157\n",
            "\n",
            "Epoch 12/50\n",
            "--------------------\n",
            "train Loss: 0.8955 Acc: 0.7640\n",
            "valid Loss: 0.6992 Acc: 0.9171\n",
            "\n",
            "Epoch 13/50\n",
            "--------------------\n",
            "train Loss: 0.9041 Acc: 0.7611\n",
            "valid Loss: 0.6881 Acc: 0.9171\n",
            "\n",
            "Epoch 14/50\n",
            "--------------------\n",
            "train Loss: 0.8959 Acc: 0.7590\n",
            "valid Loss: 0.7036 Acc: 0.9157\n",
            "\n",
            "Epoch 15/50\n",
            "--------------------\n",
            "train Loss: 0.8970 Acc: 0.7672\n",
            "valid Loss: 0.6880 Acc: 0.9171\n",
            "\n",
            "Epoch 16/50\n",
            "--------------------\n",
            "train Loss: 0.8972 Acc: 0.7653\n",
            "valid Loss: 0.6961 Acc: 0.9171\n",
            "\n",
            "Epoch 17/50\n",
            "--------------------\n",
            "train Loss: 0.8831 Acc: 0.7707\n",
            "valid Loss: 0.6906 Acc: 0.9171\n",
            "\n",
            "Epoch 18/50\n",
            "--------------------\n",
            "train Loss: 0.8848 Acc: 0.7662\n",
            "valid Loss: 0.7019 Acc: 0.9214\n",
            "\n",
            "Epoch 19/50\n",
            "--------------------\n",
            "train Loss: 0.8839 Acc: 0.7692\n",
            "valid Loss: 0.6869 Acc: 0.9157\n",
            "\n",
            "Epoch 20/50\n",
            "--------------------\n",
            "train Loss: 0.9024 Acc: 0.7614\n",
            "valid Loss: 0.6877 Acc: 0.9114\n",
            "\n",
            "Epoch 21/50\n",
            "--------------------\n",
            "train Loss: 0.8874 Acc: 0.7682\n",
            "valid Loss: 0.7013 Acc: 0.9157\n",
            "\n",
            "Epoch 22/50\n",
            "--------------------\n",
            "train Loss: 0.8927 Acc: 0.7589\n",
            "valid Loss: 0.6929 Acc: 0.9200\n",
            "\n",
            "Epoch 23/50\n",
            "--------------------\n",
            "train Loss: 0.8906 Acc: 0.7694\n",
            "valid Loss: 0.7045 Acc: 0.9157\n",
            "\n",
            "Epoch 24/50\n",
            "--------------------\n",
            "train Loss: 0.8899 Acc: 0.7689\n",
            "valid Loss: 0.6912 Acc: 0.9200\n",
            "\n",
            "Epoch 25/50\n",
            "--------------------\n",
            "train Loss: 0.8835 Acc: 0.7608\n",
            "valid Loss: 0.7013 Acc: 0.9171\n",
            "\n",
            "Epoch 26/50\n",
            "--------------------\n",
            "train Loss: 0.8915 Acc: 0.7648\n",
            "valid Loss: 0.6923 Acc: 0.9171\n",
            "\n",
            "Epoch 27/50\n",
            "--------------------\n",
            "train Loss: 0.9137 Acc: 0.7603\n",
            "valid Loss: 0.6963 Acc: 0.9086\n",
            "\n",
            "Epoch 28/50\n",
            "--------------------\n",
            "train Loss: 0.8976 Acc: 0.7586\n",
            "valid Loss: 0.6791 Acc: 0.9157\n",
            "\n",
            "Epoch 29/50\n",
            "--------------------\n",
            "train Loss: 0.9015 Acc: 0.7581\n",
            "valid Loss: 0.6837 Acc: 0.9129\n",
            "\n",
            "Epoch 30/50\n",
            "--------------------\n",
            "train Loss: 0.8832 Acc: 0.7673\n",
            "valid Loss: 0.6917 Acc: 0.9157\n",
            "\n",
            "Epoch 31/50\n",
            "--------------------\n",
            "train Loss: 0.8987 Acc: 0.7599\n",
            "valid Loss: 0.6913 Acc: 0.9086\n",
            "\n",
            "Epoch 32/50\n",
            "--------------------\n",
            "train Loss: 0.9119 Acc: 0.7556\n",
            "valid Loss: 0.6988 Acc: 0.9171\n",
            "\n",
            "Epoch 33/50\n",
            "--------------------\n",
            "train Loss: 0.9062 Acc: 0.7609\n",
            "valid Loss: 0.6971 Acc: 0.9143\n",
            "\n",
            "Epoch 34/50\n",
            "--------------------\n",
            "train Loss: 0.8887 Acc: 0.7665\n",
            "valid Loss: 0.6998 Acc: 0.9114\n",
            "\n",
            "Epoch 35/50\n",
            "--------------------\n",
            "train Loss: 0.8988 Acc: 0.7625\n",
            "valid Loss: 0.6831 Acc: 0.9214\n",
            "\n",
            "Epoch 36/50\n",
            "--------------------\n",
            "train Loss: 0.9040 Acc: 0.7589\n",
            "valid Loss: 0.6956 Acc: 0.9171\n",
            "\n",
            "Epoch 37/50\n",
            "--------------------\n",
            "train Loss: 0.8836 Acc: 0.7594\n",
            "valid Loss: 0.6834 Acc: 0.9186\n",
            "\n",
            "Epoch 38/50\n",
            "--------------------\n",
            "train Loss: 0.9156 Acc: 0.7569\n",
            "valid Loss: 0.7061 Acc: 0.9186\n",
            "\n",
            "Epoch 39/50\n",
            "--------------------\n",
            "train Loss: 0.9095 Acc: 0.7591\n",
            "valid Loss: 0.6950 Acc: 0.9186\n",
            "\n",
            "Epoch 40/50\n",
            "--------------------\n",
            "train Loss: 0.9097 Acc: 0.7536\n",
            "valid Loss: 0.6918 Acc: 0.9143\n",
            "\n",
            "Epoch 41/50\n",
            "--------------------\n",
            "train Loss: 0.9037 Acc: 0.7608\n",
            "valid Loss: 0.6835 Acc: 0.9171\n",
            "\n",
            "Epoch 42/50\n",
            "--------------------\n",
            "train Loss: 0.8955 Acc: 0.7594\n",
            "valid Loss: 0.6904 Acc: 0.9171\n",
            "\n",
            "Epoch 43/50\n",
            "--------------------\n",
            "train Loss: 0.8736 Acc: 0.7691\n",
            "valid Loss: 0.6972 Acc: 0.9171\n",
            "\n",
            "Epoch 44/50\n",
            "--------------------\n",
            "train Loss: 0.8852 Acc: 0.7649\n",
            "valid Loss: 0.6970 Acc: 0.9200\n",
            "\n",
            "Epoch 45/50\n",
            "--------------------\n",
            "train Loss: 0.8903 Acc: 0.7645\n",
            "valid Loss: 0.6887 Acc: 0.9157\n",
            "\n",
            "Epoch 46/50\n",
            "--------------------\n",
            "train Loss: 0.9035 Acc: 0.7618\n",
            "valid Loss: 0.6965 Acc: 0.9229\n",
            "\n",
            "Epoch 47/50\n",
            "--------------------\n",
            "train Loss: 0.8779 Acc: 0.7699\n",
            "valid Loss: 0.6861 Acc: 0.9143\n",
            "\n",
            "Epoch 48/50\n",
            "--------------------\n",
            "train Loss: 0.8962 Acc: 0.7663\n",
            "valid Loss: 0.6956 Acc: 0.9114\n",
            "\n",
            "Epoch 49/50\n",
            "--------------------\n",
            "train Loss: 0.8963 Acc: 0.7637\n",
            "valid Loss: 0.6858 Acc: 0.9186\n",
            "\n",
            "Epoch 50/50\n",
            "--------------------\n",
            "train Loss: 0.8780 Acc: 0.7711\n",
            "valid Loss: 0.6898 Acc: 0.9171\n",
            "Entrenamiento completado en 49m 15s\n",
            "Mejor precisión de validación: 0.9229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos los pesos del modelo entrenado\n",
        "torch.save(model_ft.state_dict(), 'resnet18_70_breeds_best_weights_v2.pth')"
      ],
      "metadata": {
        "id": "2LhWmeRAz6Zg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}